{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sQTOoeUzcs8AP2s4dVrfn3Ed7x6g3_Xc",
      "authorship_tag": "ABX9TyNWhLnmheWy/VTL9VdJWlwI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andohyung/MS_Project/blob/main/BART_%EC%9A%94%EC%95%BD%EB%AA%A8%EB%8D%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART 사용 요약 모델\n"
      ],
      "metadata": {
        "id": "lYCydxyOi9jA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d3musbJ8XMNU"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# BART 요약 모델과 토크나이저 로드\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# 요약할 파일 경로 설정\n",
        "file_path = \"/content/drive/MyDrive/멋사_실전프로젝트1/프로젝트1_입력_논문리스트/bert논문/수정1_bert논문.txt\"\n",
        "\n",
        "# 텍스트 파일 읽기\n",
        "try:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(\"파일을 찾을 수 없습니다. 경로를 확인해 주세요.\")\n",
        "    exit()\n",
        "\n",
        "# 토크나이징 및 입력 텐서로 변환\n",
        "inputs = tokenizer(text, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# 요약 생성\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], max_length=250, min_length=100, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGlqUO52YLvm",
        "outputId": "6dc771cc-f1c1-4377-daaa-654428593ffd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: BERT is designed to pre-train deep bidirectional representations fromunlabeled text by jointly conditioning on both left and right context in all layers. It obtains new state-of-the-art re-sults on eleven natural language processingtasks, including pushing the GLUE score to80.5% (7.7% point absolute improvement),MultiNLI accuracy to 86.7%, and SQuAD v1.1 question answer-ing Test F1 to 93.2 (1.5 point absolute im-provement)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROUGE 평가"
      ],
      "metadata": {
        "id": "ltvy-x_PciOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6p46xwpcnfi",
        "outputId": "8da24fc2-f43f-4abb-f8bf-45742d5fd5f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.6)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=065eeb3f58af33ea6e010d930423d50236752c993dde74a02366a953f1fdacaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "\n",
        "# BART 요약 모델과 토크나이저 로드\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# 요약할 파일 경로 설정\n",
        "file_path = \"/content/drive/MyDrive/멋사_실전프로젝트1/프로젝트1_입력_논문리스트/bert논문/수정1_bert논문.txt\"  # 파일 경로 설정\n",
        "\n",
        "# 실제 요약 텍스트 (평가할 요약의 참조용 텍스트 입력)\n",
        "reference_summary = \"BERT (Bidirectional Encoder Representations from Transformers) is a language model that learns deep bidirectional representations by considering both left and right context. It uses Masked Language Modeling and Next Sentence Prediction to improve understanding across tasks. BERT achieves state-of-the-art results on multiple NLP benchmarks with minimal task-specific adjustments, outperforming prior unidirectional models like GPT. Its open-source release has made it a cornerstone in NLP research.\"\n",
        "\n",
        "# 텍스트 파일 읽기\n",
        "try:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "except FileNotFoundError:\n",
        "    print(\"파일을 찾을 수 없습니다. 경로를 확인해 주세요.\")\n",
        "    exit()\n",
        "\n",
        "# 토크나이징 및 입력 텐서로 변환\n",
        "inputs = tokenizer(text, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# 요약 생성\n",
        "summary_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=150,      # 최대 요약 길이 설정\n",
        "    min_length=50,      # 최소 요약 길이 설정\n",
        "    length_penalty=2.0,  # 길이 페널티\n",
        "    num_beams=4,         # 빔 서치\n",
        "    early_stopping=True\n",
        ")\n",
        "generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ROUGE 점수 계산\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(reference_summary, generated_summary)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Generated Summary:\", generated_summary)\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"ROUGE-1:\", scores['rouge1'])\n",
        "print(\"ROUGE-2:\", scores['rouge2'])\n",
        "print(\"ROUGE-L:\", scores['rougeL'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz9haDDfchrW",
        "outputId": "b5a23047-6834-4a12-bdd2-2a5092627e53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary: We introduce a new language representa-tion model called BERT, which stands forBidirectional Encoder Representations fromTransformers. BERT is designed to pre-train deep bidirectional representations fromunlabeled text by jointly conditioning on both left and right context in all layers. It obtains new state-of-the-art re-sults on eleven natural language processing tasks.\n",
            "\n",
            "ROUGE Scores:\n",
            "ROUGE-1: Score(precision=0.5, recall=0.38571428571428573, fmeasure=0.435483870967742)\n",
            "ROUGE-2: Score(precision=0.18867924528301888, recall=0.14492753623188406, fmeasure=0.16393442622950818)\n",
            "ROUGE-L: Score(precision=0.37037037037037035, recall=0.2857142857142857, fmeasure=0.3225806451612903)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning 진행할 데이터"
      ],
      "metadata": {
        "id": "0D8VJUJOieM6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYUAutp0ihdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}